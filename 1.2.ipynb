{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section :1 - Adapting Architectures for resolution change:\n",
    "\n",
    "Q-1.2 – Please refer to the “Image-to-image translation using conditional adversarial nets” (pix2pix) architecture (paper, code). The task is to adapt this conditional GAN to be used as a semantic segmentor with a 960x720 resolution. Please use the CamSeq01 dataset (link) for the training / test. You can either directly adapt the lua reference implementation from the author or take any other implementation in your preferred framework and adapt it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paper Link : https://arxiv.org/pdf/1611.07004v1.pdf\n",
    "\n",
    "Dataset Link : http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamSeq01/\n",
    "\n",
    "Git Link : https://github.com/phillipi/pix2pix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective : \n",
    "\n",
    "To adapt this conditional GAN to be used as a semantic segmentor with a 960x720 resolution\n",
    "\n",
    "### Experiment : \n",
    "\n",
    "There are two ways to solve this problem :\n",
    "\n",
    "* Resize the images in the dataset to (286 * 286) dimensions and then use the same model.\n",
    "* We need to modify the image default size defined in the model in the below link.\n",
    "https://github.com/phillipi/pix2pix/blob/f74f965960e5102ff6d90694b10ee13112994d59/util/util.lua#L14\n",
    "https://github.com/phillipi/pix2pix/blob/4358ca36de75e922db7ae83e827f61a54af0706e/train.lua#L17\n",
    "https://github.com/phillipi/pix2pix/blob/cf489ddc81d14aef812aa47fd4e5d535ac6d5671/test.lua#L15\n",
    "\n",
    "\n",
    "### Experiments tried : \n",
    "\n",
    "I setup the model and ran the training and testing the model on CPU.\n",
    "\n",
    "* DATA_ROOT=./datasets/facades name=facades_generation which_direction=BtoA gpu=0 cudnn=0 batchSize=10 save_epoch_freq=5 th train.lua\n",
    "\n",
    "* DATA_ROOT=./datasets/facades name=facades_generation which_direction=BtoA phase=val th test.lua"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output log: \n",
    "\n",
    "$ DATA_ROOT=./datasets/facades name=facades_generation which_direction=BtoA gpu=0 cudnn=0 batchSize=10 save_epoch_freq=5 th train.lua\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "{\n",
    "  cudnn : 0\n",
    "  name : \"facades_generation\"\n",
    "  niter : 200\n",
    "  batchSize : 10\n",
    "  n_layers_D : 0\n",
    "  ndf : 64\n",
    "  which_model_netG : \"unet\"\n",
    "  save_display_freq : 5000\n",
    "  print_freq : 50\n",
    "  gpu : 0\n",
    "  use_GAN : 1\n",
    "  DATA_ROOT : \"./datasets/facades\"\n",
    "  serial_batch_iter : 1\n",
    "  use_L1 : 1\n",
    "  save_epoch_freq : 5\n",
    "  output_nc : 3\n",
    "  checkpoints_dir : \"./checkpoints\"\n",
    "  input_nc : 3\n",
    "  beta1 : 0.5\n",
    "  continue_train : 0\n",
    "  which_direction : \"BtoA\"\n",
    "  phase : \"train\"\n",
    "  fineSize : 256\n",
    "  condition_GAN : 1\n",
    "  loadSize : 286\n",
    "  lambda : 100\n",
    "  ngf : 64\n",
    "  preprocess : \"regular\"\n",
    "  which_model_netD : \"basic\"\n",
    "  display : 1\n",
    "  display_freq : 100\n",
    "  display_id : 10\n",
    "  flip : 1\n",
    "  ntrain : inf\n",
    "  lr : 0.0002\n",
    "  nThreads : 2\n",
    "  display_plot : \"errL1\"\n",
    "  save_latest_freq : 5000\n",
    "  serial_batches : 0\n",
    "}\n",
    "Random Seed: 4355\t\n",
    "#threads...2\t\n",
    "Starting donkey with id: 2 seed: 4357\n",
    "table: 0x40d5f080\n",
    "Starting donkey with id: 1 seed: 4356\n",
    "table: 0x41ca3a90\n",
    "./datasets/facades\n",
    "./datasets/facades\n",
    "trainCache\t/home/nitish/Desktop/DeepLearning/DeepLearningQuestions/section1/resources/pix2pix/cache/_home_nitish_Desktop_DeepLearning_DeepLearningQuestions_section1_resources_pix2pix_datasets_facades_train_trainCache.t7\n",
    "Creating train metadata\n",
    "serial batch:, \t0\n",
    "trainCache\t/home/nitish/Desktop/DeepLearning/DeepLearningQuestions/section1/resources/pix2pix/cache/_home_nitish_Desktop_DeepLearning_DeepLearningQuestions_section1_resources_pix2pix_datasets_facades_train_trainCache.t7\n",
    "Creating train metadata\n",
    "serial batch:, \t0\n",
    "table: 0x400d4990\n",
    "table: 0x418b4ca8\n",
    "running \"find\" on each class directory, and concatenate all those filenames into a single file containing all image paths for a given class\n",
    "running \"find\" on each class directory, and concatenate all those filenames into a single file containing all image paths for a given class\n",
    "now combine all the files to a single large file\n",
    "now combine all the files to a single large file\n",
    "load the large concatenated list of sample paths to self.imagePath\n",
    "cmd..wc -L '/tmp/lua_mwv4vK' |cut -f1 -d' '\n",
    "load the large concatenated list of sample paths to self.imagePath\n",
    "cmd..wc -L '/tmp/lua_tNY0Gf' |cut -f1 -d' '\n",
    " [.................................................... [.........................................................................................400 samples found.0ms           .]  ETA: 0ms | Step: 0ms         \n",
    "Updating classList and imageClass appropriately\n",
    "400 samples found.\n",
    "Updating classList and imageClass appropriately\n",
    " [======================================================================================>]                           ============================ 1/1  0ms | Step: 0ms=== Tot: 0ms | Step: 0ms==> ]                               \n",
    "1/1 \n",
    "Cleaning up temporary files\n",
    "Cleaning up temporary files\n",
    "Dataset Size: \t400\t\n",
    "define model netG...\t\n",
    "define model netD...\t\n",
    "nn.gModule\n",
    "nn.Sequential {\n",
    "  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> (8) -> (9) -> (10) -> (11) -> (12) -> (13) -> output]\n",
    "  (1): nn.SpatialConvolution(6 -> 64, 4x4, 2,2, 1,1)\n",
    "  (2): nn.LeakyReLU(0.2)\n",
    "  (3): nn.SpatialConvolution(64 -> 128, 4x4, 2,2, 1,1)\n",
    "  (4): nn.SpatialBatchNormalization (4D) (128)\n",
    "  (5): nn.LeakyReLU(0.2)\n",
    "  (6): nn.SpatialConvolution(128 -> 256, 4x4, 2,2, 1,1)\n",
    "  (7): nn.SpatialBatchNormalization (4D) (256)\n",
    "  (8): nn.LeakyReLU(0.2)\n",
    "  (9): nn.SpatialConvolution(256 -> 512, 4x4, 1,1, 1,1)\n",
    "  (10): nn.SpatialBatchNormalization (4D) (512)\n",
    "  (11): nn.LeakyReLU(0.2)\n",
    "  (12): nn.SpatialConvolution(512 -> 1, 4x4, 1,1, 1,1)\n",
    "  (13): nn.Sigmoid\n",
    "}\n",
    "running model on CPU\t\n",
    "End of epoch 1 / 200 \t Time Taken: 984.798\t\n",
    "Epoch: [2][       9 /       40]\t Time: 2.925  DataTime: 0.024    Err_G: 1.4477  Err_D: 0.5167  ErrL1: 0.3529\t\n",
    "End of epoch 2 / 200 \t Time Taken: 1685.305\t\n",
    "Epoch: [3][      19 /       40]\t Time: 4.088  DataTime: 0.000    Err_G: 1.7879  Err_D: 0.2474  ErrL1: 0.3250\t\n",
    "End of epoch 3 / 200 \t Time Taken: 1796.634\t\n",
    "^C^C\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replicating the same model in Keras (Functional API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "x = Input(shape=(100))\n",
    "conv_layer_1 = Conv2D(64,(4,4),(2,2),(1,1))(x)\n",
    "# Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1),activation=None, \n",
    "# use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros',kernel_regularizer=None, \n",
    "# bias_regularizer=None, activity_regularizer=None, kernel_constraint=None,bias_constraint=None)\n",
    "# nn.SpatialConvolution(nInputPlane, nOutputPlane, kW, kH, [dW], [dH], [padW], [padH])\n",
    "activation_layer_1 = LeakyReLU(alpha=0.2)(conv_layer_1)\n",
    "\n",
    "conv_layer_2 = Conv2D(128,(4,4),(2,2),(1,1))(activation_layer_1)\n",
    "norm_layer_1 = BatchNormalization(axis=1)(conv_layer_2)\n",
    "# (axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', \n",
    "# gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones',\n",
    "# beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)\n",
    "activation_layer_2 = LeakyReLU(alpha=0.2)(norm_layer_1)\n",
    "\n",
    "conv_layer_3 = Conv2D(256,(4,4),(2,2),(1,1))(activation_layer_2)\n",
    "norm_layer_2 = BatchNormalization(axis=1)(conv_layer_3)\n",
    "activation_layer_3 = LeakyReLU(alpha=0.2)(norm_layer_2)\n",
    "\n",
    "conv_layer_4 = Conv2D(512,(4,4),(2,2),(1,1))(activation_layer_3)\n",
    "norm_layer_3 = BatchNormalization(axis=1)(conv_layer_4)\n",
    "activation_layer_4 = LeakyReLU(alpha=0.2)(norm_layer_3)\n",
    "\n",
    "conv_layer_5 = Conv2D(1,(4,4),(2,2),(1,1))(activation_layer_4)\n",
    "y= Dense(1, activation='sigmoid')(conv_layer_5)\n",
    "\n",
    "\n",
    "pix2pix_model = Model(input=x, output = y)\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "pix2pix_model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
    "\n",
    "pix2pix_model.fit(x_train, y_train, batch_size=32, epochs=10)\n",
    "score = model.evaluate(x_test, y_test, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer architecture defined in Torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> (8) -> (9) -> (10) -> (11) -> (12) -> (13) -> output]\n",
    "  (1): nn.SpatialConvolution(6 -> 64, 4x4, 2,2, 1,1)\n",
    "  (2): nn.LeakyReLU(0.2)\n",
    "  (3): nn.SpatialConvolution(64 -> 128, 4x4, 2,2, 1,1)\n",
    "  (4): nn.SpatialBatchNormalization (4D) (128)\n",
    "  (5): nn.LeakyReLU(0.2)\n",
    "  (6): nn.SpatialConvolution(128 -> 256, 4x4, 2,2, 1,1)\n",
    "  (7): nn.SpatialBatchNormalization (4D) (256)\n",
    "  (8): nn.LeakyReLU(0.2)\n",
    "  (9): nn.SpatialConvolution(256 -> 512, 4x4, 1,1, 1,1)\n",
    "  (10): nn.SpatialBatchNormalization (4D) (512)\n",
    "  (11): nn.LeakyReLU(0.2)\n",
    "  (12): nn.SpatialConvolution(512 -> 1, 4x4, 1,1, 1,1)\n",
    "  (13): nn.Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
